{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=[\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "# env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x4\", is_slippery=False, render_mode='human')\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x4\", is_slippery=False)\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom rewards\n",
    "custom_rewards = {\n",
    "    'S': 0.0,  # Reward for frozen tiles (very small positive reward)\n",
    "    'F': -0.75,  # Reward for falling in a hole (negative reward)\n",
    "    'G': 1.0,   # Reward for reaching the goal (the \"gift\" state)\n",
    "}\n",
    "\n",
    "# Map custom rewards to the environment's reward table\n",
    "env.env.rewards = custom_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom policy to avoid edges\n",
    "def custom_policy(state):\n",
    "    if state % 4 == 0:  # Agent is at leftmost column\n",
    "        return [1, 2, 3]  # Avoid going left\n",
    "    elif state % 4 == 3:  # Agent is at rightmost column\n",
    "        return [0, 1, 3]  # Avoid going right\n",
    "    elif state < 4:  # Agent is at top row\n",
    "        return [0, 1, 2]  # Avoid going up\n",
    "    elif state > 15:  # Agent is at bottom row\n",
    "        return [0, 2, 3]  # Avoid going down\n",
    "    else:\n",
    "        return [0, 1, 2, 3]  # All actions are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "epsilon = 1.0\n",
    "max_exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "num_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_successrates = []\n",
    "train_episodes = []\n",
    "train_time = []\n",
    "test_successrates = []\n",
    "test_steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [08:48<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for test in tqdm(range(1000), desc='Training Progress'):\n",
    "    # Initialize Q-table with zeros\n",
    "    Q = np.random.rand(env.observation_space.n, env.action_space.n) * 0.01\n",
    "    numSuccesses = 0\n",
    "    converged = False\n",
    "    episode = 0\n",
    "    start_time = time.time()  # Start time of training\n",
    "    while not converged and episode < num_episodes:\n",
    "        episode += 1\n",
    "        state_tuple = env.reset()  # State is a tuple\n",
    "        state = state_tuple[0]  # Extract the integer state value\n",
    "        done = False\n",
    "\n",
    "        # Reset state visits count for the new episode\n",
    "        state_visits = {s: 0 for s in range(env.observation_space.n)}\n",
    "\n",
    "        while not done:\n",
    "            # Choose action using epsilon-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(custom_policy(state))  # Custom policy\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])\n",
    "\n",
    "            # print(\"Episode:\", episode)\n",
    "            # print(\"Action:\", action)\n",
    "\n",
    "            # Take action and observe the next state, reward, done flag, and info\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            next_state = step_result[0]  # Extract the next state tuple\n",
    "            reward = step_result[1]  # Extract the reward\n",
    "            terminated = step_result[2]  # Extract the done flags\n",
    "            truncated = step_result[3] # Extract the done flags\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Update state visits count\n",
    "            state_visits[next_state] += 1\n",
    "\n",
    "            # Calculate penalty for visiting the same state\n",
    "            visit_penalty = -0.01 * (2 ** state_visits[next_state])\n",
    "\n",
    "            # Check if the agent stayed in the same state\n",
    "            if next_state == state:\n",
    "                reward = visit_penalty\n",
    "            else:\n",
    "                # Check for falling into the ice\n",
    "                if terminated and reward == 0:\n",
    "                    reward = custom_rewards[\"F\"]  # Penalty for falling into the ice\n",
    "                elif done and reward > 0.9:\n",
    "                    numSuccesses += 1\n",
    "                elif not terminated:\n",
    "                    reward = custom_rewards[\"S\"]  # Reward for a safe move\n",
    "                reward += visit_penalty  # Add penalty for repeated visits\n",
    "\n",
    "            prev_Q = Q.copy()  # Store previous Q-values for convergence check\n",
    "\n",
    "            # Update Q-value using SARSA formula\n",
    "            next_action = np.argmax(Q[next_state, :])\n",
    "            Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "            # Check for convergence (e.g., small change in Q-values)\n",
    "            if np.sum(np.abs(Q - prev_Q)) < 0.00025:\n",
    "                converged = True\n",
    "                break\n",
    "\n",
    "            # Decay the exploration rate\n",
    "            epsilon = max(min_exploration_rate, epsilon * exploration_decay_rate)\n",
    "\n",
    "            # print(\"Step Result:\", step_result)\n",
    "            # print(\"State Tuple:\", state_tuple)\n",
    "            # print(\"State:\", state)\n",
    "            # print(\"New State:\", next_state)\n",
    "            # print(\"Next Action:\", next_action)\n",
    "            # print(\"Reward:\", reward)\n",
    "            # print(\"Done:\", done)\n",
    "            # print(\"New Q-Value\", Q[state, action])\n",
    "            # print(\"--------NEXT--------\")\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    end_time = time.time()  # End time of training\n",
    "    training_time = end_time - start_time  # Calculate training time in seconds\n",
    "\n",
    "    if converged:\n",
    "        train_episodes.append(episode)\n",
    "    else:\n",
    "        train_episodes.append(num_episodes)\n",
    "\n",
    "    train_time.append(training_time)\n",
    "    trainSuccessRate = numSuccesses/num_episodes\n",
    "    train_successrates.append(trainSuccessRate)\n",
    "\n",
    "    test_sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sarsa():\n",
    "    numSuccesses = 0\n",
    "    numTestEpisodes = 10000\n",
    "    steps = 0\n",
    "    for episode in range(numTestEpisodes):\n",
    "        state_tuple = env.reset()  # State is a tuple\n",
    "        state = state_tuple[0]  # Extract the integer state value\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            steps += 1  # Increment step counter for each step taken\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "            # Take action and observe the next state, reward, done flag, and info\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            next_state = step_result[0]  # Extract the next state tuple\n",
    "            reward = step_result[1]\n",
    "            terminated = step_result[2]  # Extract the done flags\n",
    "            truncated = step_result[3] # Extract the done flags\n",
    "            done = terminated or truncated\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done and reward > 0.9:\n",
    "                numSuccesses += 1\n",
    "\n",
    "    averageSteps = steps/numTestEpisodes\n",
    "    testSuccessRate = numSuccesses/numTestEpisodes\n",
    "    test_successrates.append(testSuccessRate)\n",
    "    test_steps.append(averageSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Train Episodes': train_episodes, 'Train Success Rates': train_successrates, 'Train Time': train_time, 'Test Steps': test_steps, 'Test Success Rates': test_successrates})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
