{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=[\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "# env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x4\", is_slippery=False, render_mode='human')\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x4\", is_slippery=False)\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom rewards\n",
    "custom_rewards = {\n",
    "    'S': 0.0,  # Reward for frozen tiles (very small positive reward)\n",
    "    'F': -0.75,  # Reward for falling in a hole (negative reward)\n",
    "    'G': 1.0,   # Reward for reaching the goal (the \"gift\" state)\n",
    "}\n",
    "\n",
    "# Map custom rewards to the environment's reward table\n",
    "env.env.rewards = custom_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom policy to avoid edges\n",
    "def custom_policy(state):\n",
    "    if state % 4 == 0:  # Agent is at leftmost column\n",
    "        return [1, 2, 3]  # Avoid going left\n",
    "    elif state % 4 == 3:  # Agent is at rightmost column\n",
    "        return [0, 1, 3]  # Avoid going right\n",
    "    elif state < 4:  # Agent is at top row\n",
    "        return [0, 1, 2]  # Avoid going up\n",
    "    elif state > 15:  # Agent is at bottom row\n",
    "        return [0, 2, 3]  # Avoid going down\n",
    "    else:\n",
    "        return [0, 1, 2, 3]  # All actions are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate average rewards over a window of episodes\n",
    "def calculate_average_rewards(rewards_list, window_size):\n",
    "    if len(rewards_list) < window_size:\n",
    "        return None  # Insufficient data to calculate average\n",
    "    else:\n",
    "        return np.mean(rewards_list[-window_size:])  # Calculate average over the specified window\n",
    "\n",
    "# Specify convergence criteria based on average rewards stability\n",
    "def check_convergence(rewards_list, window_size, threshold):\n",
    "    average_rewards = calculate_average_rewards(rewards_list, window_size)\n",
    "    if average_rewards is not None:\n",
    "        return np.abs(average_rewards - rewards_list[-1]) < threshold  # Check if average rewards change is below threshold\n",
    "    else:\n",
    "        return False  # Insufficient data for convergence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.9\n",
    "max_exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "num_episodes = 1000\n",
    "window_size = 200  # Number of episodes to consider for calculating average rewards\n",
    "threshold = 0.05  # Threshold for considering average rewards stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_successrates = []\n",
    "train_episodes = []\n",
    "train_time = []\n",
    "test_successrates = []\n",
    "test_steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for test in tqdm(range(10), desc='Training Progress'):\n",
    "    # Initialize Q-table with zeros\n",
    "    Q = np.random.rand(env.observation_space.n, env.action_space.n) * 0.01\n",
    "    numSuccesses = 0\n",
    "    converged = False\n",
    "    episode = 0\n",
    "    rewards_list = []\n",
    "    start_time = time.time()  # Start time of training\n",
    "    while not converged and episode < num_episodes:\n",
    "        episode += 1\n",
    "        state_tuple = env.reset()  # State is a tuple\n",
    "        state = state_tuple[0]  # Extract the integer state value\n",
    "        done = False\n",
    "        # Reset state visits count for the new episode\n",
    "        state_visits = {s: 0 for s in range(env.observation_space.n)}\n",
    "\n",
    "        while not done:\n",
    "            # Choose action using epsilon-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(custom_policy(state))  # Custom policy\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])\n",
    "\n",
    "            # print(\"Episode:\", episode)\n",
    "            # print(\"Action:\", action)\n",
    "\n",
    "            # Take action and observe the next state, reward, done flag, and info\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            next_state = step_result[0]  # Extract the next state tuple\n",
    "            reward = step_result[1]  # Extract the reward\n",
    "            terminated = step_result[2]  # Extract the done flags\n",
    "            truncated = step_result[3] # Extract the done flags\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Update state visits count\n",
    "            state_visits[next_state] += 1\n",
    "\n",
    "            # Calculate penalty for visiting the same state\n",
    "            visit_penalty = -0.01 * (2 ** state_visits[next_state])\n",
    "\n",
    "            # Check if the agent stayed in the same state\n",
    "            if next_state == state:\n",
    "                reward = visit_penalty\n",
    "            else:\n",
    "                # Check for falling into the ice\n",
    "                if terminated and reward == 0:\n",
    "                    reward = custom_rewards[\"F\"]  # Penalty for falling into the ice\n",
    "                elif done and reward > 0.9:\n",
    "                    numSuccesses += 1\n",
    "                elif not terminated:\n",
    "                    reward = custom_rewards[\"S\"]  # Reward for a safe move\n",
    "                reward += visit_penalty  # Add penalty for repeated visits\n",
    "\n",
    "            prev_Q = Q.copy()  # Store previous Q-values for convergence check\n",
    "\n",
    "            # Update Q-value using SARSA formula\n",
    "            next_action = np.argmax(Q[next_state, :])\n",
    "            Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "            rewards_list.append(reward)  # Append episode reward to the list\n",
    "\n",
    "            # Check for convergence based on average rewards\n",
    "            if check_convergence(rewards_list, window_size, threshold):\n",
    "                converged = True\n",
    "                break\n",
    "\n",
    "            # Decay the exploration rate\n",
    "            epsilon = max(min_exploration_rate, epsilon * exploration_decay_rate)\n",
    "\n",
    "            # print(\"Step Result:\", step_result)\n",
    "            # print(\"State Tuple:\", state_tuple)\n",
    "            # print(\"State:\", state)\n",
    "            # print(\"New State:\", next_state)\n",
    "            # print(\"Next Action:\", next_action)\n",
    "            # print(\"Reward:\", reward)\n",
    "            # print(\"Done:\", done)\n",
    "            # print(\"New Q-Value\", Q[state, action])\n",
    "            # print(\"--------NEXT--------\")\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    end_time = time.time()  # End time of training\n",
    "    training_time = end_time - start_time  # Calculate training time in seconds\n",
    "\n",
    "    train_episodes.append(episode)\n",
    "    train_time.append(training_time)\n",
    "    trainSuccessRate = numSuccesses/num_episodes\n",
    "    train_successrates.append(trainSuccessRate)\n",
    "\n",
    "    test_sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sarsa():\n",
    "    numSuccesses = 0\n",
    "    numTestEpisodes = 10000\n",
    "    steps = 0\n",
    "    for episode in range(numTestEpisodes):\n",
    "        state_tuple = env.reset()  # State is a tuple\n",
    "        state = state_tuple[0]  # Extract the integer state value\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            steps += 1  # Increment step counter for each step taken\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "            # Take action and observe the next state, reward, done flag, and info\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            next_state = step_result[0]  # Extract the next state tuple\n",
    "            reward = step_result[1]\n",
    "            terminated = step_result[2]  # Extract the done flags\n",
    "            truncated = step_result[3] # Extract the done flags\n",
    "            done = terminated or truncated\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done and reward > 0.9:\n",
    "                numSuccesses += 1\n",
    "\n",
    "    averageSteps = steps/numTestEpisodes\n",
    "    testSuccessRate = numSuccesses/numTestEpisodes\n",
    "    test_successrates.append(testSuccessRate)\n",
    "    test_steps.append(averageSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Train Episodes': train_episodes, 'Train Success Rates': train_successrates, 'Train Time': train_time, 'Test Steps': test_steps, 'Test Success Rates': test_successrates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Episodes</th>\n",
       "      <th>Train Success Rates</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Test Steps</th>\n",
       "      <th>Test Success Rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005004</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train Episodes  Train Success Rates  Train Time  Test Steps  \\\n",
       "0              27                0.012    0.004001         6.0   \n",
       "1              27                0.014    0.004014         6.0   \n",
       "2              26                0.013    0.004000         6.0   \n",
       "3              26                0.010    0.005004         6.0   \n",
       "4              25                0.012    0.007000         6.0   \n",
       "5              27                0.015    0.006004         6.0   \n",
       "6              24                0.011    0.006000         6.0   \n",
       "7              26                0.014    0.002999         6.0   \n",
       "8              26                0.011    0.003002         6.0   \n",
       "9              24                0.013    0.003038         6.0   \n",
       "\n",
       "   Test Success Rates  \n",
       "0                 1.0  \n",
       "1                 1.0  \n",
       "2                 1.0  \n",
       "3                 1.0  \n",
       "4                 1.0  \n",
       "5                 1.0  \n",
       "6                 1.0  \n",
       "7                 1.0  \n",
       "8                 1.0  \n",
       "9                 1.0  "
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('sarsa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
