{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Environment\n",
    "#This is to setup a specific map\n",
    "#desc=[\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "#env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x5\", is_slippery=False, render_mode='human')\n",
    "\n",
    "#env = gym.make('FrozenLake-v1', desc = generate_random_map(size=5), is_slippery = False, render_mode = 'human')\n",
    "#observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    return gym.make('FrozenLake-v1', is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(env):\n",
    "    return np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_exploration_rates():\n",
    "    return 1.0, 1.0, 0.01, 0.001  # exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table, exploration_rate, env):\n",
    "    if random.uniform(0, 1) > exploration_rate:\n",
    "        action = np.argmax(q_table[state, :])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, new_state, q_table, learning_rate, discount_factor):\n",
    "    q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
    "                             learning_rate * (reward + discount_factor * np.max(q_table[new_state, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, q_table, learning_rate, discount_factor, exploration_rates, num_episodes):\n",
    "    exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate = exploration_rates\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state_info = env.reset()\n",
    "        state = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the integer state\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, q_table, exploration_rate, env)\n",
    "            state_info, reward, _, done, info = env.step(action)  # Adjusted unpacking here\n",
    "            new_state = state_info[0] if isinstance(state_info, tuple) else state_info\n",
    "    \n",
    "            update_q_table(state, action, reward, new_state, q_table, learning_rate, discount_factor)\n",
    "            state = new_state\n",
    "\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "                           (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, q_table, num_test_episodes):\n",
    "    total_rewards = 0\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        state_info = env.reset()\n",
    "        state = state_info[0] if isinstance(state_info, tuple) else state_info\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "            state_info, reward, _, done, info = env.step(action)  # Adjusted unpacking here\n",
    "            state = state_info[0] if isinstance(state_info, tuple) else state_info\n",
    "            episode_rewards += reward\n",
    "\n",
    "        total_rewards += episode_rewards\n",
    "\n",
    "    average_reward = total_rewards / num_test_episodes\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 1 test episodes: 0.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize the environment\n",
    "    env = create_environment()\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    q_table = initialize_q_table(env)\n",
    "\n",
    "    # Set the learning rate and discount factor\n",
    "    learning_rate = 0.8\n",
    "    discount_factor = 0.95\n",
    "\n",
    "    # Initialize exploration rates\n",
    "    exploration_rates = initialize_exploration_rates()\n",
    "\n",
    "    # Set the number of episodes for training and testing\n",
    "    num_training_episodes = 100\n",
    "    num_test_episodes = 1\n",
    "\n",
    "    # Train the agent\n",
    "    train_agent(env, q_table, learning_rate, discount_factor, exploration_rates, num_training_episodes)\n",
    "\n",
    "    # Evaluate the agent\n",
    "    average_reward = evaluate_agent(env, q_table, num_test_episodes)\n",
    "    print(f\"Average Reward over {num_test_episodes} test episodes: {average_reward}\")\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m exploration_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     32\u001b[0m max_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m---> 34\u001b[0m episode_reached \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_until_goal\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoal reached at episode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reached\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 12\u001b[0m, in \u001b[0;36mtrain_until_goal\u001b[1;34m(env, q_table, learning_rate, discount_factor, exploration_rate, max_episodes)\u001b[0m\n\u001b[0;32m      9\u001b[0m new_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Take the action\u001b[39;00m\n\u001b[0;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()  \u001b[38;5;66;03m# Render the new state\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m q_table[state, action] \u001b[38;5;241m=\u001b[39m \u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m discount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_table[new_state, :]) \u001b[38;5;241m-\u001b[39m q_table[state, action])\n\u001b[0;32m     13\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:  \u001b[38;5;66;03m# Check if the episode ended\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def train_until_goal(env, q_table, learning_rate, discount_factor, exploration_rate, max_episodes):\n",
    "    for episode in range(max_episodes):\n",
    "        env.reset()  # Start a new episode\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state, :]) if np.random.rand() > exploration_rate else env.action_space.sample()\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)  # Take the action\n",
    "\n",
    "            q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "            state = new_state\n",
    "\n",
    "            if terminated or truncated:  # Check if the episode ended\n",
    "                done = True\n",
    "                if terminated and reward == 1.0:  # Check if the goal was reached\n",
    "                    print(f\"Goal reached at episode {episode} in {episode_steps} steps.\")\n",
    "                    return episode\n",
    "\n",
    "        exploration_rate = max(exploration_rate * 0.99, 0.01)  # Decrement the exploration rate\n",
    "\n",
    "    return -1  # Return -1 if the goal was not reached within the max episodes\n",
    "\n",
    "# Usage\n",
    "desc = [\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False, render_mode='human')  \n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_rate = 1.0\n",
    "max_episodes = 1000\n",
    "\n",
    "episode_reached = train_until_goal(env, q_table, learning_rate, discount_factor, exploration_rate, max_episodes)\n",
    "env.close()\n",
    "print(f\"Goal reached at episode: {episode_reached}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0, False, False, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "desc = [\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False, render_mode='human')  \n",
    "env.reset()\n",
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 1.0, True, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)\n",
    "#obs, reward, terminated, truncated , info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
